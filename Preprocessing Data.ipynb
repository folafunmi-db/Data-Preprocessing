{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/folafunmi/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[5.1, -2.9, 3.3], \n",
    "                       [-1.2, 7.8, -6.1], \n",
    "                       [3.9, 0.4, 2.1], \n",
    "                       [7.3, -9.9, -4.5]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binarization\n",
    "This process is used when we want to convert our numerical values into boolean values.\n",
    "Using 2.1 as the threshold, all values above it would become 1 and the rest becomes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean Removal\n",
    "This is done so that each feature is centered around zero. It helps to remove bias from the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before:\n",
      "Mean =  [ 3.775 -1.15  -1.3  ]\n",
      "Standard Deviation =  [3.12039661 6.36651396 4.0620192 ]\n"
     ]
    }
   ],
   "source": [
    "#print mean and standard deviation\n",
    "print(\"\\nBefore:\")\n",
    "print(\"Mean = \", input_data.mean(axis = 0))\n",
    "print(\"Standard Deviation = \", input_data.std(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After:\n",
      "Mean =  [1.11022302e-16 0.00000000e+00 2.77555756e-17]\n",
      "Standard Deviation =  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\nAfter:\")\n",
    "print(\"Mean = \", data_scaled.mean(axis = 0))\n",
    "print(\"Standard Deviation = \", data_scaled.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean is closer to zero and the standard deviation is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to bring the values of different features to within the same range, such that features with naturally large measurements wouldn't overwhelm others with smaller measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Data: \n",
      " [[ 5.1 -2.9  3.3]\n",
      " [-1.2  7.8 -6.1]\n",
      " [ 3.9  0.4  2.1]\n",
      " [ 7.3 -9.9 -4.5]]\n",
      "\n",
      "Min max scaled data: \n",
      " [[0.74117647 0.39548023 1.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.6        0.5819209  0.87234043]\n",
      " [1.         0.         0.17021277]]\n"
     ]
    }
   ],
   "source": [
    "#Min max scaling\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "\n",
    "print(\"\\nInput Data: \\n\", input_data)\n",
    "print(\"\\nMin max scaled data: \\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row is scaled such that the max value is 1 and all other values are relative to this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modifies the values in the feature vector so that we can measure them on a common scale.\n",
    "\n",
    "Some common forms of normalization aim to modify the values so that they sum to 1: \n",
    "\n",
    ">L1 normalization, which refers to Least Absolute Deviation. This is such that the sum of absolute values is a row is 1.\n",
    "\n",
    ">L2 normalization which refers to least square makes sure the sum of squares is 1\n",
    "\n",
    "In general, L1 normalization technique is considered more robust than L2 normalization technique. L1 normalization technique is robust because it is resistant to outliers in the data. If we are solving a problem where outliers are important, then maybe L2 normalization becomes a better choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input data:\n",
      " [[ 5.1 -2.9  3.3]\n",
      " [-1.2  7.8 -6.1]\n",
      " [ 3.9  0.4  2.1]\n",
      " [ 7.3 -9.9 -4.5]]\n",
      "\n",
      "L1 normalized data:\n",
      " [[ 0.45132743 -0.25663717  0.2920354 ]\n",
      " [-0.0794702   0.51655629 -0.40397351]\n",
      " [ 0.609375    0.0625      0.328125  ]\n",
      " [ 0.33640553 -0.4562212  -0.20737327]]\n",
      "\n",
      "L2 normalized data:\n",
      " [[ 0.75765788 -0.43082507  0.49024922]\n",
      " [-0.12030718  0.78199664 -0.61156148]\n",
      " [ 0.87690281  0.08993875  0.47217844]\n",
      " [ 0.55734935 -0.75585734 -0.34357152]]\n"
     ]
    }
   ],
   "source": [
    "#Normalize data\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm=\"l1\")\n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm=\"l2\")\n",
    "\n",
    "print(\"\\nInput data:\\n\", input_data)\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)\n",
    "print(\"\\nL2 normalized data:\\n\", data_normalized_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming the Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of absolute values of L1 normalized row 1:\n",
      "0.9999999999999998\n",
      "Sum of absolute values of L1 normalized row 2:\n",
      "1.0\n",
      "Sum of absolute values of L1 normalized row 3:\n",
      "1.0\n",
      "Sum of absolute values of L1 normalized row 4:\n",
      "1.0\n",
      "\n",
      "\n",
      "Sum of absolute values of L2 normalized row 1:\n",
      "0.9999999999999998\n",
      "Sum of absolute values of L2 normalized row 2:\n",
      "1.0\n",
      "Sum of absolute values of L2 normalized row 3:\n",
      "1.0\n",
      "Sum of absolute values of L2 normalized row 4:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#fact check for l1 normalization\n",
    "for a in range(len(data_normalized_l1)):\n",
    "    abs_val = []\n",
    "    for val in data_normalized_l1[a]:\n",
    "        abs_val.append(abs(val))\n",
    "    print(f\"Sum of absolute values of L1 normalized row {a+1}:\")\n",
    "    print(sum(abs_val))\n",
    "    \n",
    "#fact check for l2 normalization\n",
    "print('\\n')\n",
    "for a in range(len(data_normalized_l2)):\n",
    "    abs_val = []\n",
    "    for val in data_normalized_l1[a]:\n",
    "        abs_val.append(abs(val))\n",
    "    print(f\"Sum of absolute values of L2 normalized row {a+1}:\")\n",
    "    print(sum(abs_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, there are usually a lot of labels which can be in form of words, numbers etc. In sklearn, the functions expect these labels as numbers unlike in reality where are more likely human readable words.\n",
    ">Label encoding involves transforming the word labels into numerical forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample input labels\n",
    "input_labels = ['red', 'black', 'red', 'green', 'black', 'yellow', 'white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create label encoder and fit the labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(input_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping:\n",
      "black  -->  0\n",
      "green  -->  1\n",
      "red  -->  2\n",
      "white  -->  3\n",
      "yellow  -->  4\n"
     ]
    }
   ],
   "source": [
    "#Print the mapping\n",
    "print('\\nLabel mapping:')\n",
    "for i, item in enumerate(encoder.classes_):\n",
    "    print(item, \" --> \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` This was done in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels:  ['green', 'red', 'black']\n",
      "Encoded values:  [1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#Encode a set of labels using the encoder\n",
    "test_labels = ['green', 'red', 'black']\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "print(\"\\nLabels: \", test_labels)\n",
    "print(\"Encoded values: \", list(encoded_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded values:  [3, 0, 1, 4]\n",
      "Decoded values:  ['white' 'black' 'green' 'yellow']\n"
     ]
    }
   ],
   "source": [
    "#Decode a set of labels using the encoder\n",
    "encoded_values = [3, 0, 1, 4]\n",
    "decoded_list = encoder.inverse_transform(encoded_values)\n",
    "print(\"\\nEncoded values: \", encoded_values)\n",
    "print(\"Decoded values: \", decoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
